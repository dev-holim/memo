### 데이터 불러오기 (와인 품질 분류 데이터셋)
```python
import pandas as pd  
import numpy as np  
  
train = pd.read_csv(f"{DATA_PATH}winequality_train.csv") # 학습데이터  
test = pd.read_csv(f"{DATA_PATH}winequality_test.csv") # 테스트 데이터
```

- 특성으로 사용할 변수 추가
```python
cols = ['산도', '휘발성산', '시트르산', '잔당', '염화물', '독립이산화황', '총이산화황', '밀도', '수소이온농도','황산염', '도수']  
train_ft = train[cols].copy()  
test_ft = test[cols].copy()
```

- 종류 컬럼 0, 1로 인코딩하여 피쳐 추가
```python
# red 와인일경우 1 아닐경우 0
train_ft["종류"] = train["종류"].map(lambda x : int(x == "red") )  
test_ft["종류"] = test["종류"].map(lambda x : int(x == "red") )
```

- Min-Max Scaling
```python
from sklearn.preprocessing import MinMaxScaler  
scaler = MinMaxScaler()
scaler.fit(train_ft)

train_ft[train_ft.columns] = scaler.transform(train_ft) # 학습 데이터
```

## 앙상블 학습(Ensemble Learning)
> - 기계학습에서 여러개의 개별모델의 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
> - 모델별로 편향성은 존재할수 밖에 없다. 앙상블을 통해 보완함으로써 성능 향상
> - 앙상블 학습의 유형
>	보팅(Voting), 스태킹(Stacking), 배깅(Bagging), 부스팅(Boosting)

### 1. Voting
- 여러 모델들의 예측값을 투표방식(hard) or 평균방식(soft) 으로 앙상블
- 분류, 회귀 모두 사용가능
```python
# cv 객체 생성  
from sklearn.model_selection import StratifiedKFold  
cv = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)

# 모델  
from sklearn.linear_model import LogisticRegression  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score  
from sklearn.ensemble import VotingClassifier
```


- VotingClassifier 주요 파라미터
	- estimators: 모델 객체를 리스트에 담아 전달하면 된다.
	- voting: 'hard'(default) , 'soft'
```python
# 별칭과 모델 객체를 튜플에 담아서 리스트에 넣음  
estimators = [
	("knn",KNeighborsClassifier() ),
    ("lr",LogisticRegression(random_state=42)),
    ("dt",DecisionTreeClassifier(random_state=42)),
]  
  
# 딕셔너리를 언패킹하여 키워드 아규먼트 전달 방식  
params ={  
    "estimators" : estimators, # 모델 담은 리스트  
    "voting" : "soft" , # soft 는 각 모델의 예측확률 평균  
    "n_jobs" : -1  
}  
  
model = VotingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
params ={  
    "estimators" : estimators, # 모델 담은 리스트  
    "voting" : "hard" , # hard 는 과반수 투표  
    "n_jobs" : -1  
}  
  
model = VotingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model.fit(train_ft,target)
pred = model.predict(test_ft)
```


### 2. Stacking
> - 여러 모델들의 예측 결과를 학습 데이터로 만들어서 다시 별도의 최종 모델(메타 모델)로 재 학습시켜 예측하는 방식​  
> - 과적합 방지를 위해 내부적으로 각모델별로 CV 를 진행한다.  
> - 분류, 회귀 모두 사용가능  
> - StackingClassifier 주요 파라미터  
> 	- estimators: 모델 객체를 리스트에 담아 전달하면 된다.
> 	- final_estimator: 메타 모델 객체 전달 
> 	- cv: 정수(fold 수) 혹은 cv 객체 전달

```python
from sklearn.ensemble import StackingClassifier  
  
# 딕셔너리를 언패킹하여 키워드 아규먼트 전달 방식  
params ={  
    "estimators" : estimators, # 모델 담은 리스트  
    "final_estimator" : LogisticRegression(random_state=42,max_iter=500) , # 메타 모델  
    "cv" : cv, # cv 객체  
    "n_jobs" : -1  
}  
  
model = StackingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model = StackingClassifier(**params)
model.fit(train_ft,target)
pred = model.predict(test_ft)
```


### 3. Bagging
> - Bootstrap aggregating의 약자로 Bootstrap sampling 기법을 이용해 학습데이터를 여러 번 뽑아 각 모델에 학습시켜 예측 결과의 최빈값 또는 평균으로 예측하는 방식
> - Bootstrap sampling
> 	- 크기가 n인 학습데이터가 주어졌을 때 하나의 샘플이 나올 확률은 1/n 으로 하고 랜덤하게 샘플들을 뽑아 새로운 학습데이터를 여러 개 만드는 기법으로 한 번 뽑힌 샘플을 복원하여 다시 뽑는 복원 추출법이 사용
> - 분류, 회귀 모두 사용가능
> - BaggingClassifier 주요 파라미터
> 	- random_state
> 		- 시드값
> 	- estimator
> 		- 모델 객체를 전달하면 된다.
> 		- 기본값은 None이며 tree 모델이 선택된다.
> 	- n_estimators
> 		- base 모델 개수(Bootstrap sampling 수)
> 		- 기본값은 10
> 	- max_samples
> 		- 각 모델이 학습시 사용할 샘플 개수
> 		- int(개수) or float(비율) 형으로 주면 된다. 기본값 1.0
> 	- max_features
> 		- 각 모델이 학습시 사용할 피쳐 개수
> 		- int(개수) or float(비율) 형으로 주면 된다. 기본값 1.0
> 	- bootstrap_features
> 		- 각 모델별로 피쳐를 다르게 할지 여부
> 		- bool 자료형을 넣으면 되고 기본값은 False

```python
from sklearn.ensemble import BaggingClassifier  
  
params ={  
    "random_state" : 42,  
    "estimator" : LogisticRegression(random_state=42) , # None 이면 결정트리 사용함  
    'n_estimators':10, # base estimators 개수  
    "n_jobs" : -1  
}  
model = BaggingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model = BaggingClassifier(**params)  
model.fit(train_ft,target)  
pred = model.predict(test_ft)
```


### 4. Random Forest
> - 다수의 결정 트리 모델이 학습데이터에서 **배깅 방식**으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 앙상블 하는 모델
> - 각 결정 트리 모델의 예측 결과의 상관 관계를 줄이고 다양성을 주기위해 전체 특성을 사용하지 않고 각 트리별로 랜덤하게 m개의 특성을 선택하여 학습
> - 분류, 회귀 모두 사용가능
> - RandomForestClassifier 주요 파라미터
> 	- random_state: 시드값
> 	- n_estimators: tree 모델 개수(Bootstrap sampling 수)
> 	- max_samples: 각 트리모델이 학습시 사용할 샘플 개수
```python
from sklearn.ensemble import RandomForestClassifier  
  
params ={  
    "random_state" : 42,  
    'n_estimators':80, # 트리 개수  
    'min_samples_split':4, # 노드를 분할하는데 필요한 최소 샘플수  
    "n_jobs" : -1  
}  
model = RandomForestClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model = RandomForestClassifier(**params)  
model.fit(train_ft,target)  
pred = model.predict(test_ft)
```


### 5. Gradient Boosting
> - 트리 기반의 Boosting 계열의 앙상블 모델
> - 부스팅(Boosting)
> 	- 여러 개의 약한 학습기를 순차적으로 학습하면서 약한 학습기들을 결합하여 강한 분류기를 만드는 과정
> - 분류, 회귀 모두 사용가능
> - 데이터가 적으면 과적합이 발생한다.
> - GradientBoostingClassifier 주요 파라미터
> 	- random_state: 시드값
> 	- n_estimators: 부스팅 단계수
> 	- learning_rate: 학습률로 부스팅 과정에서 각 트리의 기여도
![[image 26.png]]

```python
from sklearn.ensemble import GradientBoostingClassifier  
  
params ={  
    "random_state" : 42,  
    'n_estimators': 100, # 수행할 부스팅 단계의 수  
    "learning_rate": 0.5 # 학습률  
}
model = GradientBoostingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```


```python
from sklearn.metrics import mean_squared_error # mse
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

diabetes = load_diabetes() # 데이터셋 받아오기
x = diabetes.data # 학습데이터
y = diabetes.target # 정답데이터

x_train, x_valid, y_train, y_valid = train_test_split(x, y,random_state=42)
```

#### 5-1. 트리모델을 이용하여 Gradient Boosting방식으로 학습
```python
hp = {  
   "max_depth" : 2,  
   "random_state" : 42  
}
```

- 1단계: 실제값을 타겟으로 하여 학습
```python
from sklearn.tree import DecisionTreeRegressor # 트리 회귀

weak_1 = DecisionTreeRegressor(**hp)  
weak_1.fit(x_train, y_train)  
pred = weak_1.predict(x_train) # 오차를 구하기 위해 학습데이터로 예측
```

- 2단계: 실제값과 이전단계 예측값의 차이를 타겟으로 하여 학습
```python
residual = y_train - pred # 오차 구하기  
weak_2 = DecisionTreeRegressor(**hp)  
weak_2.fit(x_train, residual) # 오차를 타겟으로 설정  
pred = weak_2.predict(x_train) # 오차를 구하기 위해 학습데이터로 예측
```

- 3단계: 이전단계 타겟값과 이전단계 예측값의 차이를 타겟으로 하여 학습
```python
residual = residual - pred # 오차 구하기  
weak_3 = DecisionTreeRegressor(**hp)  
weak_3.fit(x_train, residual) # 오차를 타겟으로 설정
```

- 4단계: 증셋에 대하여 평가해보기
```python
pred = weak_1.predict(x_valid) + weak_2.predict(x_valid) + weak_3.predict(x_valid)  
mean_squared_error(y_valid, pred) ** 0.5 # rmse
```


#### 5-2. Gradient Boosting 모델 학습
```python
hp["n_estimators"] = 3 # 수행할 부스팅 단계의 수  
hp["learning_rate"] = 1.0 # 학습률
```

```python
from sklearn.ensemble import GradientBoostingRegressor # gbm 회귀

gbr = GradientBoostingRegressor(**hp)  
gbr.fit(x_train, y_train)  
pred = gbr.predict(x_valid)  
  
mean_squared_error(y_valid, pred) ** 0.5 # rmse
```


### 6. XGBoost
> - GBM의 느린 수행시간을 개선하고 과적합 규제 등 다양한 기능이 지원되는 GBM 기반 라이브러리
> - 머신러닝 과제에서 뛰어난 예측 성능을 보임
> - tree 기반의 앙상블 학습에서 각광받는 알고리즘
> - 분류, 회귀 모두 사용가능
> - XGBClassifier 주요 파라미터
> 	- random_state: 시드값
> 	- n_estimators: 부스팅 단계수, 기본값은 100
> 	- learning_rate: 학습률, 기본값은 0.1
> 	- objective: 학습 문제 정의
> 	- eval_metric: 검증셋에 대한 평가지표
> 	- early_stopping_rounds: 조기 종료 조건
```python
!pip install xgboost
!pip install -U xgboost
```

```python
from xgboost import XGBClassifier  
  
params ={  
    "random_state" : 42,  
    "objective" : 'multi:softmax',  
    'n_estimators': 300, # 수행할 부스팅 단계의 수  
    'learning_rate' : 0.1,  
    'n_jobs' : -1  
}  
model = XGBClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```


```python
#%% md  
- 학습 제어 주요 파라미터  
  - eval_set  
      - 검증 데이터를 넣는다  
      - 검증셋을 튜플에 담고 튜플을 리스트에 담아 전달  
#%%  
from sklearn.metrics import accuracy_score  
  
params ={  
    "random_state" : 42,  
    "objective" : 'multi:softmax',  
    'n_estimators': 1000, # 수행할 부스팅 단계의 수  
    'learning_rate' : 0.2,  
    'n_jobs' : -1,  
    "early_stopping_rounds": 50, # xgboost 최신버전에 경우 XGBClassifier 클래스에 인수로 전달해야 함  
    "eval_metric": "mlogloss" # xgboost 최신버전에 경우 XGBClassifier 클래스에 인수로 전달해야 함  
}  
  
scores = [] # 각 폴드의 검증점수 저장할 리스트  
for tri , vai in cv.split(train_ft,target):  
    # 학습및 검증 데이터  
    x_train = train_ft.iloc[tri]  
    y_train = target.iloc[tri]  
    x_valid = train_ft.iloc[vai]  
    y_valid = target.iloc[vai]  
  
    model = XGBClassifier(**params)  
    model = model.fit(x_train, y_train,eval_set=[(x_valid,y_valid)]) # 학습  
    pred = model.predict(x_valid)  # 예측  
    score = accuracy_score(y_valid,pred) # 검증셋 평가  
    scores.append(score) # 리스트에 저장  
  
print(scores)  
np.mean(scores) # 검증점수 평균
```


```python
params ={  
    "random_state" : 42,  
    "objective" : 'multi:softmax',  
    'n_estimators': 300, # 수행할 부스팅 단계의 수  
    'learning_rate' : 0.1,  
    'n_jobs' : -1  
}  
model = XGBClassifier(**params)  
model.fit(train_ft,target)
```

- 시각화
```python
import matplotlib.pyplot as plt  
from xgboost import plot_importance  
plot_importance(model)  
plt.show()
```



### 7. LightGBM
> - 대용량 데이터 처리에 적합
> - 다른 GBM 계열의 알고리즘보다 메모리를 적게 사용하고 학습속도가 빠름
> - 데이터 수가 너무 적을 때는 과적합 문제
> - XGBoost 보다 학습에 걸리는 시간이 적음
> - 일반적인 트리 계열 알고리즘과 다르게 LightGBM은 리프 중심 트리 분할을 사용
> - 분류, 회귀 모두 사용가능
![[image 27.png]]
```python
!pip install lightgbm
```

```python
from lightgbm import LGBMClassifier  
  
params ={  
    "random_state" : 42,  
    "objective" : 'multiclass',  
    'n_estimators': 300, # 수행할 부스팅 단계의 수  
    'learning_rate' : 0.1,  
    'n_jobs' : -1  
}  
model = LGBMClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model = LGBMClassifier(**params)  
model.fit(train_ft,target)
```

- 시각화
```python
from lightgbm import plot_importance  
plot_importance(model)  
plt.show()
```



### 8. Catboost
> - 범주형 변수에 강력한 성능을 보여주는 부스팅 모델
> - 범주형 변수가 많을경우 높은 성능과 함께 학습 속도가 개선된다.
> - 수치형 변수가 대부분일 때는 매우 느리다.
> - 범주형 변수를 인코딩 하지 않고 넣어도 된다.
```python
!pip install catboost
```

```python
from catboost import CatBoostClassifier  
  
params ={  
    "random_state" : 42,  
    'n_estimators': 300, # 수행할 부스팅 단계의 수  
    "loss_function" : "MultiClass",  
}  
model = CatBoostClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
train_ft = train.iloc[:,:-1].copy()  
train_ft.head()
```

```python
params ={  
    "random_state" : 42,  
    'n_estimators': 300, # 수행할 부스팅 단계의 수  
    "loss_function" : "MultiClass",  
    "cat_features" : ["종류"],  
    "one_hot_max_size" : 1  
}  
model = CatBoostClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```