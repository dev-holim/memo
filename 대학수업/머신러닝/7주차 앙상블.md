### 데이터 불러오기 (와인 품질 분류 데이터셋)
```python
import pandas as pd  
import numpy as np  
  
train = pd.read_csv(f"{DATA_PATH}winequality_train.csv") # 학습데이터  
test = pd.read_csv(f"{DATA_PATH}winequality_test.csv") # 테스트 데이터
```

- 특성으로 사용할 변수 추가
```python
cols = ['산도', '휘발성산', '시트르산', '잔당', '염화물', '독립이산화황', '총이산화황', '밀도', '수소이온농도','황산염', '도수']  
train_ft = train[cols].copy()  
test_ft = test[cols].copy()
```

- 종류 컬럼 0, 1로 인코딩하여 피쳐 추가
```python
# red 와인일경우 1 아닐경우 0
train_ft["종류"] = train["종류"].map(lambda x : int(x == "red") )  
test_ft["종류"] = test["종류"].map(lambda x : int(x == "red") )
```

- Min-Max Scaling
```python
from sklearn.preprocessing import MinMaxScaler  
scaler = MinMaxScaler()
scaler.fit(train_ft)

train_ft[train_ft.columns] = scaler.transform(train_ft) # 학습 데이터
```

## 앙상블 학습(Ensemble Learning)
> - 기계학습에서 여러개의 개별모델의 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
> - 모델별로 편향성은 존재할수 밖에 없다. 앙상블을 통해 보완함으로써 성능 향상
> - 앙상블 학습의 유형
>	보팅(Voting), 스태킹(Stacking), 배깅(Bagging), 부스팅(Boosting)

### 1. Voting
- 여러 모델들의 예측값을 투표방식(hard) or 평균방식(soft) 으로 앙상블
- 분류, 회귀 모두 사용가능
```python
# cv 객체 생성  
from sklearn.model_selection import StratifiedKFold  
cv = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)

# 모델  
from sklearn.linear_model import LogisticRegression  
from sklearn.tree import DecisionTreeClassifier  
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score  
from sklearn.ensemble import VotingClassifier
```


- VotingClassifier 주요 파라미터
	- estimators: 모델 객체를 리스트에 담아 전달하면 된다.
	- voting: 'hard'(default) , 'soft'
```python
# 별칭과 모델 객체를 튜플에 담아서 리스트에 넣음  
estimators = [
	("knn",KNeighborsClassifier() ),
    ("lr",LogisticRegression(random_state=42)),
    ("dt",DecisionTreeClassifier(random_state=42)),
]  
  
# 딕셔너리를 언패킹하여 키워드 아규먼트 전달 방식  
params ={  
    "estimators" : estimators, # 모델 담은 리스트  
    "voting" : "soft" , # soft 는 각 모델의 예측확률 평균  
    "n_jobs" : -1  
}  
  
model = VotingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
params ={  
    "estimators" : estimators, # 모델 담은 리스트  
    "voting" : "hard" , # hard 는 과반수 투표  
    "n_jobs" : -1  
}  
  
model = VotingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model.fit(train_ft,target)
pred = model.predict(test_ft)
```


### 2. Stacking
> - 여러 모델들의 예측 결과를 학습 데이터로 만들어서 다시 별도의 최종 모델(메타 모델)로 재 학습시켜 예측하는 방식​  
> - 과적합 방지를 위해 내부적으로 각모델별로 CV 를 진행한다.  
> - 분류, 회귀 모두 사용가능  
> - StackingClassifier 주요 파라미터  
> 	- estimators: 모델 객체를 리스트에 담아 전달하면 된다.
> 	- final_estimator: 메타 모델 객체 전달 
> 	- cv: 정수(fold 수) 혹은 cv 객체 전달

```python
from sklearn.ensemble import StackingClassifier  
  
# 딕셔너리를 언패킹하여 키워드 아규먼트 전달 방식  
params ={  
    "estimators" : estimators, # 모델 담은 리스트  
    "final_estimator" : LogisticRegression(random_state=42,max_iter=500) , # 메타 모델  
    "cv" : cv, # cv 객체  
    "n_jobs" : -1  
}  
  
model = StackingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model = StackingClassifier(**params)
model.fit(train_ft,target)
pred = model.predict(test_ft)
```


### 3. Bagging
> - Bootstrap aggregating의 약자로 Bootstrap sampling 기법을 이용해 학습데이터를 여러 번 뽑아 각 모델에 학습시켜 예측 결과의 최빈값 또는 평균으로 예측하는 방식
> - Bootstrap sampling
> 	- 크기가 n인 학습데이터가 주어졌을 때 하나의 샘플이 나올 확률은 1/n 으로 하고 랜덤하게 샘플들을 뽑아 새로운 학습데이터를 여러 개 만드는 기법으로 한 번 뽑힌 샘플을 복원하여 다시 뽑는 복원 추출법이 사용
> - 분류, 회귀 모두 사용가능
> - BaggingClassifier 주요 파라미터
> 	- random_state
> 		- 시드값
> 	- estimator
> 		- 모델 객체를 전달하면 된다.
> 		- 기본값은 None이며 tree 모델이 선택된다.
> 	- n_estimators
> 		- base 모델 개수(Bootstrap sampling 수)
> 		- 기본값은 10
> 	- max_samples
> 		- 각 모델이 학습시 사용할 샘플 개수
> 		- int(개수) or float(비율) 형으로 주면 된다. 기본값 1.0
> 	- max_features
> 		- 각 모델이 학습시 사용할 피쳐 개수
> 		- int(개수) or float(비율) 형으로 주면 된다. 기본값 1.0
> 	- bootstrap_features
> 		- 각 모델별로 피쳐를 다르게 할지 여부
> 		- bool 자료형을 넣으면 되고 기본값은 False

```python
from sklearn.ensemble import BaggingClassifier  
  
params ={  
    "random_state" : 42,  
    "estimator" : LogisticRegression(random_state=42) , # None 이면 결정트리 사용함  
    'n_estimators':10, # base estimators 개수  
    "n_jobs" : -1  
}  
model = BaggingClassifier(**params)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='accuracy',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```

```python
model = BaggingClassifier(**params)  
model.fit(train_ft,target)  
pred = model.predict(test_ft)
```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```