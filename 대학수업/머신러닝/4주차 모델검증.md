#### 범주형 변수 원핫인코딩하여 특성 추가

```python
from sklearn.preprocessing import OneHotEncoder  
cols = ['gender','embarked']  
enc = OneHotEncoder(handle_unknown = 'ignore') # 모르는 범주가 있을 경우 무시  
enc.fit(train_ft[cols])
```

- 학습 데이터
```python
tmp = pd.DataFrame(  
    enc.transform(train_ft[cols]).toarray(), # ndarray  
    columns = enc.get_feature_names_out() # 컬럼명  
)  
train_ft = pd.concat([train_ft,tmp],axis=1)  
```

- 테스트 데이터
```python
tmp = pd.DataFrame(  
    enc.transform(test_ft[cols]).toarray(),  
    columns = enc.get_feature_names_out()  
)  
test_ft = pd.concat([test_ft,tmp],axis=1)
```

- Min-Max Scaling
```python
from sklearn.preprocessing import MinMaxScaler  
scaler = MinMaxScaler()  
scaler.fit(train_ft)
```

```python
train_ft[train_ft.columns] = scaler.transform(train_ft)
test_ft[test_ft.columns] = scaler.transform(test_ft)
```


### Holdout 방식 검증
> ![[image 23.png]]
> - 데이터를 2개의 세트로 분리하는 방식 (학습데이터, 검증데이터)
> - 빠른 속도로 모델 검증 가능
> - 학습데이터의 전체를 검증 할 수 없는 단점이 있음

```python
from sklearn.model_selection import train_test_split  
  
x_train, x_valid, y_train, y_valid = train_test_split(train_ft, target, random_state=42 , test_size = 0.3,shuffle=True)
```

```python
from sklearn.tree import DecisionTreeClassifier  
model = DecisionTreeClassifier(max_depth=10,random_state=42)  
model.fit(x_train, y_train)  
pred = model.predict_proba(x_valid)[:,1]
```

```python
from sklearn.metrics import roc_auc_score  
roc_auc_score(y_valid,pred)
```


### K-Fold 방식 교차검증
> ![[image 22.png]]
> - 전체데이터를 k등분하고, 각 등분을 한번씩 검증데이터로 사용하여, 성능평가를 하고 각 폴드의 성능값들의 평균을 통해 검증하는 방법
> - 학습데이터의 전체 부분을 검증 할 수 있음
> - K번의 학습과 검증이 진행되므로 각 검증점수의 평균을 검증결과로 사용

```python
from sklearn.model_selection import KFold
cv = KFold(n_splits=5,shuffle=True, random_state=42) # shuffle 및 시드고정 할 것  
gen = cv.split(train_ft) # split 메소드를 실행하면 제너레이터가 반환된다.
next(gen)
```

```python
scores = [] # 각 폴드의 검증점수 저장할 리스트  
for tri , vai in gen:  
    # 학습및 검증 데이터  
    x_train = train_ft.iloc[tri]  
    y_train = target.iloc[tri]  
    x_valid = train_ft.iloc[vai]  
    y_valid = target.iloc[vai]  
  
    model = DecisionTreeClassifier(max_depth=10,random_state=42)  
    model = model.fit(x_train, y_train) # 학습  
    pred = model.predict_proba(x_valid)[:,1]  # 예측  
    score = roc_auc_score(y_valid,pred) # 검증셋 평가  
    print(score) # 폴드별 점수  
    scores.append(score) # 리스트에 저장
```

### Stratified K-Fold 방식 교차검증
> - ![[image 24.png]]
> - 타겟의 클래스 비율을 유지하며 K개의 세트로 분리하는 방식
> - K-fold 과정에서 검증세트에 특정 클래스가 몰려 있거나 특정 클래스의 비율이 매우 낮다면 검증세트에 포함되지 않는 경우 등의 단점을 보완

```python
from sklearn.model_selection import StratifiedKFold  
  
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  
for tri, vai in cv.split(train_ft,target):
    x_train = train_ft.iloc[tri]
	y_train = target.iloc[tri]
	x_valid = train_ft.iloc[vai]
	y_valid = target.iloc[vai]

	model = DecisionTreeClassifier(max_depth=10,random_state=42) 
	model = model.fit(x_train, y_train) # 학습
	pred = model.predict_proba(x_valid)[:,1]  # 예측
	score = roc_auc_score(y_valid,pred) # 검증셋 평가
	print(score) # 폴드별 점수
	scores.append(score) # 리스트에 저장
```



### cross_val_score 함수를 이용한 교차검증
```python
model = DecisionTreeClassifier(max_depth=10,random_state=42) # 모델  
```

```python
cv = KFold(n_splits=5, shuffle=True, random_state=42) # cv 객체  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='roc_auc',n_jobs = -1) # -1을 줄경우 모든 코어 활용
```

```python
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # cv 객체  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='neg_log_loss',n_jobs = -1)
```