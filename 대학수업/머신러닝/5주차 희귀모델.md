- 데이터 불러오기(보험료 예측 데이터)
	- age: 나이
	- sex: 성별  
	- bmi: 체질량 지수  
	- children: 자녀수  
	- smoker: 흡연여부  
	- region: 지역  
	- target:의료비
```python
import pandas as pd
import numpy as np

train = pd.read_csv(f"{DATA_PATH}insurance_train.csv") # 학습데이터
test = pd.read_csv(f"{DATA_PATH}insurance_test.csv") # 테스트 데이터
```

- 특성으로 사용할 변수 추가하기
```python
cols = ["age","bmi","children"]  
train_ft = train[cols].copy()  
test_ft = test[cols].copy()

target = train["target"]
```

- sex, smoker 컬럼 0과1로 인코딩하여 피쳐추가
```python
train_ft["sex"] = train["sex"].map(lambda x : int(x == "male") )  
train_ft["smoker"] = train["smoker"].map(lambda x : int(x == "yes"))  
  
test_ft["sex"] = test["sex"].map(lambda x : int(x == "male") )  
test_ft["smoker"] = test["smoker"].map(lambda x : int(x == "yes"))
```

- 범주형 변수 원핫인코딩하여 특성으로 추가하기
```python
from sklearn.preprocessing import OneHotEncoder  
cols = ['region']  
enc = OneHotEncoder(handle_unknown = 'ignore') # 모르는 범주가 있을 경우 무시  
enc.fit(train[cols])
```

```python
tmp = pd.DataFrame(  
    enc.transform(train[cols]).toarray(), # ndarray  
    columns = enc.get_feature_names_out() # 컬럼명  
)  
train_ft = pd.concat([train_ft,tmp],axis=1)  
train_ft.head()

# 테스트 데이터  
tmp = pd.DataFrame(  
    enc.transform(test[cols]).toarray(),  
    columns = enc.get_feature_names_out()  
)  
test_ft = pd.concat([test_ft,tmp],axis=1)  
test_ft.head()
```

- Min-Max Scaling
```python
from sklearn.preprocessing import MinMaxScaler  
scaler = MinMaxScaler()  
scaler.fit(train_ft)

train_ft[train_ft.columns] = scaler.transform(train_ft) # 학습 데이터
test_ft[test_ft.columns] = scaler.transform(test_ft) # 테스트 데이터
```



### LinearRegression 클래스
> - 선형 회귀(Linear regression) 모델
> - 원인이 되는 설명변수(독립변수,Feature)에 따른 종속변수(목표변수, target)의 결과 예측
> $$  
y = b_0 + b_1x  
$$  
> $$  
y = b_0 + b_1x_1 +  b_2x_2 + ... + b_nx_n  
$$  
> - 최소제곱법을 이용한 선형회귀
> 	- RSS를 최소화하는 회귀 계수를 선택하는 통계적 접근법
> 	- RSS를 각각의 회귀계수들로 미분했을때 0이 되는 해를 구하는 방법  
> $$  
b_1 = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}  
$$  
> $$  
b_0 = \bar{y}-b_1\bar{x}  
$$

```python
from sklearn.model_selection import KFold  
from sklearn.model_selection import cross_val_score  
from sklearn.linear_model import LinearRegression  
  
cv = KFold(n_splits=5,shuffle=True, random_state=42) # shuffle 및 시드고정 할 것  
model = LinearRegression()
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='neg_root_mean_squared_error',n_jobs = -1)
-scores # 폴드별 검증점수 리스트
```

```python
model = LinearRegression()  
model.fit(train_ft,target) # 학습데이터 전체 학습

model.coef_ # 가중치

model.intercept_ # 편향

pred = model.predict(test_ft) # 테스트 데이터 예측
```

### Ridge 클래스  
> - 최소제곱법을 이용한 선형회귀의 L2 규제를 적용한 모델
> - 가중치가 큰 피쳐일수록 페널티가 더 가해져서 오버피팅 억제하는 효과
> - alpha 값을 이용하여 가중치를 규제하는데 alpha 값이 클수록 가중치에 규제가 더 가해져서 가중치가 감소

```python
from sklearn.linear_model import Ridge  
model = Ridge(alpha=1.0) # 1.0 기본값  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='neg_root_mean_squared_error',n_jobs = -1)  
-scores # 폴드별 검증점수 리스트
```

### Lasso 클래스  
> - 최소제곱법을 이용한 선형회귀의 L1 규제를 적용한 모델
> - 가중치가 작은 피쳐들의 가중치를 0으로 수렴하게 만들어 특성 선택을 하는 효과
> - alpha 값을 이용하여 가중치를 규제하는데 alpha 값이 클수록 가중치에 규제가 더 가해져서 가중치가 감소
```python
from sklearn.linear_model import Lasso  
model = Lasso(alpha=1.0) # 1.0 기본값  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='neg_root_mean_squared_error',n_jobs = -1)  
-scores # 폴드별 검증점수 리스트
```

### ElasticNet 클래스  
> - 최소제곱법을 이용한 선형회귀의 L1, L2 규제를 조합하여 적용한 모델
> - L1 규제는 alpha값에 따라 회귀 계수의 값이 급격히 변동하여 많은 회귀 계수들을  0으로 만드는 성향이 강해 이를 완화하기 위해 L2 규제를 추가한 것이 엘라스틱넷
> - l1_ratio 파라미터 : L1 규제의 비율
```python
from sklearn.linear_model import ElasticNet  
model = ElasticNet(alpha=1.0,l1_ratio = 0.9)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='neg_root_mean_squared_error',n_jobs = -1)  
-scores # 폴드별 검증점수 리스트
```


## 분류문제
### LogisticRegression 클래스
> - 선형회귀 + 시그모이드 함수
> - 경사하강법을 이용하여 모델 파라미터를 업데이트
> - 예측의 결정에 $\sigma$(시그모이드) 함수를 사용
> - 입력값이 클수록 1에 가깝게 출력, 입력값이 작을수록 0에 가깝게 출력
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state=42)
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='roc_auc',n_jobs = -1)

np.mean(scores) # cv 평균 점수
```

- L1규제 적용
```python
model = LogisticRegression(random_state=42,penalty="l1",solver="liblinear")  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='roc_auc',n_jobs = -1)  

np.mean(scores) # cv 평균 점수
```

- 규제 적용 X
```python
model = LogisticRegression(random_state=42,penalty=None) # 사이킷런 1.2 버전 이상일경우에는 None 을 penalty에 인수로 줘야함  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='roc_auc',n_jobs = -1)
np.mean(scores) # cv 평균 점수
```

- L2 규제 강도 올리기
```python
model = LogisticRegression(random_state=42,C=0.2)  
scores = cross_val_score(model,train_ft,target,cv = cv ,scoring='roc_auc',n_jobs = -1)  
print(scores) # 폴드별 검증점수 리스트  
np.mean(scores) # cv 평균 점수
```


- 테스트 데이터에 대해서 모델 평가
```python
from sklearn.metrics import roc_auc_score  
  
model = LogisticRegression(random_state=42,C=0.2)  
model.fit(train_ft,target) #학습 데이터 전체 다시학습  
  
# 테스트 데이터 예측 및 평가  
y_test = test_target["survived"] # 테스트셋 y값  
pred = model.predict_proba(test_ft)[:,1] # 예측  
roc_auc_score(y_test,pred) # AUC 평가
```