> 실제값과 모델에 의해 예측된 값을 비교하여 모델의 성능을 측정 하는 것
> 모델 평가

### LinearRegression

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train, y_train) # 학습
pred = model.predict(x_valid) # 예측
```


### 회귀 평가

- MSE
$$  
MSE = \frac{1}{n}{\sum_{i=1}^{n}(y_i-\hat{y}_i)^2}  
$$
```python
from sklearn.metrics import mean_squared_error  
mse = mean_squared_error(y_valid,pred)
```

- RMSE
$$  
RMSE = \sqrt{\frac{1}{n}{\sum_{i=1}^{n}(y_i-\hat{y}_i)^2}}  
$$
```python
np.sqrt(mse) , mse ** 0.5
```

- MAE
$$  
MAE = \frac{1}{n}{\sum_{i=1}^{n}|y_i-\hat{y}_i|}  
$$
```python
from sklearn.metrics import mean_absolute_error  
mean_absolute_error(y_valid,pred)
```

- MAPE
$$  
MAPE = \frac{100}{n}{\sum_{i=1}^{n}\frac{|y_i-\hat{y}_i|}{|y|}}  
$$
```python
def mape(true, pred):  
    return np.mean(np.abs((true - pred) / np.abs(true))) # 100(상수) 곱하는거는 생략
mape(y_valid, pred)
```

```python
from sklearn.metrics import mean_absolute_percentage_error  
mean_absolute_percentage_error(y_valid,pred)
```


### 혼동 행렬(Confusion Matrix)

- Dummy
```python
from sklearn.dummy import DummyClassifier

dummy = DummyClassifier(strategy='most_frequent')
dummy.fit(x_train, y_train) # 학습
pred_dummy = dummy.predict(x_valid) # 예측결과 저장
```

- Logistic Regression
```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=700,random_state=42)
lr.fit(x_train, y_train)
pred_lr = lr.predict(x_valid)
```


### 정확도 평가

- Accuracy
```python
from sklearn.metrics import accuracy_score  
print("dummy:", accuracy_score(y_valid, pred_dummy))  
print("lr:", accuracy_score(y_valid, pred_lr))
```

- Precision(FP를 줄이는 것이 목표일 때)
```python
from sklearn.metrics import precision_score 
precision_score(y_valid,pred_dummy), precision_score(y_valid,pred_lr)
```

- Recall(FN을 줄이는 것이 목표일 때)
```python
from sklearn.metrics import recall_score  
recall_score(y_valid,pred_dummy), recall_score(y_valid,pred_lr)
```

- F1-score
$$  
F1 = 2*\frac{Precision*Recall}{Precision+Recall}  
$$
```python
from sklearn.metrics import f1_score  
  
f1_score(y_valid,pred_dummy) , f1_score(y_valid,pred_lr)
```


- Recall & Precision & F1 한번에 보기
```python
from sklearn.metrics import classification_report

print(classification_report(y_valid, pred_lr))
```


### ROC curve

> FPR(1-specificity)를 x축으로, TPR(recall)을 y축으로 하여 둘 간의 관계를 표현한 그래프
> - FPR(False Positive Rate): FP / (FP + TN)
> - TPR(True Positive Rate): TP / (FN + TP) , 재현율 recall

```python
# 1확률만 가져오기
pred_lr = lr.predict_proba(x_valid)[:,1]

from sklearn.metrics import RocCurveDisplay  
fig, ax = plt.subplots()  
RocCurveDisplay.from_predictions(y_valid, pred_lr, ax=ax)  
plt.show()
```


### AUROC (ROC AUC)

> ROC curve의 밑부분 면적
> 넓을수록 모형 성능이 높아짐
> 임계값이 어떻게 선택되었는지와 무관하게 모델의 예측 품질을 측정
> - Poor model (0.5 ~ 0.7)
> - Fair model (0.7 ~ 0.8)
> - Good model (0.8 ~ 0.9)
> - Excellent model (0.9 ~ 1.0)

```python
from sklearn.metrics import roc_auc_score  
roc_auc_score(y_valid, pred_lr)
```


### Multi classification 에서의 F1-score

> - micro: 전체 클래스의 대하여 TP/FP/FN 구한뒤에 F1-score 계산
> - macro: 각 클래스의 대하여 F1-score 계산후 산술평균
> - Weighted: 각 클래스의 대하여 F1-score 계산후 각 클래스가 차지하는 비율에 따라 가중평균

### 결정의사나무

```python
from sklearn.tree import DecisionTreeClassifier  
tree = DecisionTreeClassifier(random_state=42,max_depth=5)  
tree = tree.fit(x_train, y_train)  
pred_tree = tree.predict(x_valid)
```

```python
from sklearn.metrics import f1_score  
f1_score(y_valid, pred_tree, average='micro')
f1_score(y_valid, pred_tree, average='macro')
f1_score(y_valid, pred_tree, average='weighted')
```


### Multi classification 에서의 logloss

> - 모델이 예측한 확률 값을 반영하여 평가 한다.
> - 0에 가까울 수록 좋은 모델이다.
> - 정답에 해당하는 확률값들을 음의 로그함수에 넣어서 나온 값들을 평균내서 평가하는데 확률값이 낮을 수록 패널티를 부여하기 위함

```python
from sklearn.metrics import log_loss  
pred = tree.predict_proba(x_valid)
log_loss(y_valid,pred)
```
